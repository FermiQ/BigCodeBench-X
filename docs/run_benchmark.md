# run_benchmark.py

## Overview

The `run_benchmark.py` script is the primary tool for evaluating language models on the BigCodeBench-X dataset. It orchestrates the process of generating code solutions for given programming problems and/or executing these solutions within a Docker container to assess their correctness. The script leverages `dspy-ai` for language model interaction (via LiteLLM) and `asyncio` for managing concurrent operations, making it efficient for handling numerous problems and model requests.

It operates via two main subcommands:
1.  **`generate`**: Takes a set of problems, a specified language model, and target programming language instructions. It prompts the model to produce code solutions and identify necessary libraries. If a Docker container name is also provided, it can subsequently execute these generated solutions.
2.  **`execute`**: Takes a file containing previously generated solutions and runs them in a specified Docker container, recording the outcomes (e.g., exit code, stdout, stderr).

The script is designed to be configurable, allowing users to specify model parameters, concurrency limits, and target environments.

## Key Components

### DSPy Signature

- **`SolveProblem(dspy.Signature)`**:
  - Defines the structure of the prompt sent to the language model.
  - **InputFields**:
    - `programming_language: str`: The target programming language for the solution.
    - `problem_statement: str`: The description of the programming problem.
  - **OutputFields**:
    - `program: str`: The code solution generated by the model.
    - `libraries: List[str]`: A list of libraries the model deems necessary for the solution.
  - This signature is wrapped in `dspy.ChainOfThought(SolveProblem)` (as `solve_problem`) to encourage the model to "think step-by-step."

### TypedDicts

- **`Problem(TypedDict)`**:
  - Represents the structure of a problem loaded from the dataset.
  - Fields: `task_id: str`, `prompt: str` (problem statement), `test_suite: str`.

### Core Functions

- **`save_output(output_path: Path, generations: List[Awaitable[dict]])`**:
  - Asynchronously writes a list of dictionaries (results from generation or execution) to the specified `output_path` in JSONL format (one JSON object per line). Each dictionary is awaited before writing.

- **`run_executions(container_name: str, num_concurrent_requests: int, generations: List[Awaitable[dict]]) -> List[asyncio.Task]`**:
  - Manages the asynchronous execution of programs within a Docker container.
  - Uses an `asyncio.Semaphore` to limit the number of concurrent Docker processes to `num_concurrent_requests`.
  - For each input `generation` (an awaitable dictionary containing `task_id`, `program`, `test_suite`):
    - It constructs a JSON payload with these details.
    - Executes `docker run --rm -i <container_name>`, piping the JSON payload to the container's stdin.
    - Captures `stdout`, `stderr`, and the `returncode` from the container.
    - Parses the JSON output from the container's `stdout` (expected to contain execution results like `exit_code`).
    - Returns a new dictionary merging the original generation data with the execution outcome. If container errors or JSON decoding errors occur, it populates error fields.
  - Returns a list of `asyncio.Task` objects, each representing a pending execution.

- **`execute_with_args(container_name: str, input_path: Path, output_path: Path, num_concurrent_requests: int)`**:
  - Implements the logic for the `execute` subcommand.
  - Reads generated solutions (JSONL) from `input_path`. Each line is wrapped in an `immediate_future` to be awaitable.
  - Calls `run_executions` to run these solutions in the specified `container_name`.
  - Calls `save_output` to write the execution results to `output_path`.

- **`generate_with_args(model_name: str, temperature: float, max_tokens: int, num_concurrent_requests: int, lang: str, container_name: Optional[str], output_path: Path)`**:
  - Implements the logic for the `generate` subcommand.
  - Initializes the `dspy.LM` with the provided `model_name` (LiteLLM compatible), `temperature`, `top_p` (hardcoded to 0.95), and `max_tokens`.
  - Performs a test call to the LM.
  - Uses an `asyncio.Semaphore` to limit concurrent requests to the LM.
  - Loads programming problems from the `nuprl-staging/BigCodeBench-MultiPL` dataset ("default" config, "test" split).
  - For each problem:
    - Asynchronously invokes `solve_problem.aforward(...)` to get the `program` and `libraries` from the LM.
    - Cleans the extracted `program` string using `extract_code_from_markdown` (from `bcb_multipl_util`).
    - Includes metadata (model name, lang, temp, etc.) and any reasoning/chain-of-thought from the model in the result.
    - Handles potential exceptions during the generation process, recording error information.
  - If `container_name` is provided, it then calls `run_executions` to execute the newly generated programs.
  - Calls `save_output` to write the generation results (and execution results, if applicable) to `output_path`.

- **`main()`**:
  - Sets up command-line argument parsing using `argparse`, defining `generate` and `execute` subparsers and their respective arguments.
  - Dispatches to `generate_with_args` or `execute_with_args` based on the subcommand chosen by the user.
  - Uses `asyncio.run(main())` in the `if __name__ == "__main__":` block to start the asyncio event loop.

- **`immediate_future(value) -> Awaitable`**:
  - A utility function that wraps a pre-resolved `value` into an `asyncio.Future` that is already completed. This is used to make non-awaitable values (like those read directly from a file in `execute_with_args`) compatible with functions expecting awaitables.

## Important Variables/Constants

- **Command-Line Arguments**:
  - **Common for both subcommands**:
    - `num_concurrent_requests` (int, default: 20): Limits concurrent operations (LM calls or Docker runs).
    - `output_path` (Path, required): Specifies the file to save results.
  - **`generate` subcommand**:
    - `--model-name` (str, required): Model identifier for LiteLLM (e.g., "gpt-3.5-turbo").
    - `--temperature` (float, default: 0.6): Sampling temperature for the LM.
    - `--max-tokens` (int, default: 5000): Maximum number of tokens for the LM to generate.
    - `--lang` (str, required): The target programming language for code generation.
    - `--container-name` (str, optional): If provided, generated code will be executed in this Docker container.
  - **`execute` subcommand**:
    - `--container-name` (str, required): The Docker container image name for executing solutions.
    - `--input-path` (Path, required): Path to a JSONL file containing solutions generated previously (e.g., by the `generate` command).
- **Dataset**: Problems are loaded from `datasets.load_dataset("nuprl-staging/BigCodeBench-MultiPL", "default", split="test")`.
- **`metadata` Dictionary**: In `generate_with_args`, a dictionary holding common parameters (`model_name`, `lang`, `temperature`, etc.) is created and merged into each output record for traceability.

## Usage Examples

**1. Generate solutions (Python) using GPT-3.5-Turbo and save to `generations_py.jsonl`:**
```bash
python bigcodebench_multipl/src/bigcodebench_multipl/run_benchmark.py generate \
    --model-name "gpt-3.5-turbo" \
    --lang "python" \
    --temperature 0.2 \
    --num-concurrent-requests 10 \
    --output-path "generations_py.jsonl"
```

**2. Generate solutions and immediately execute them using a specified Docker container:**
```bash
python bigcodebench_multipl/src/bigcodebench_multipl/run_benchmark.py generate \
    --model-name "gpt-4" \
    --lang "javascript" \
    --temperature 0.5 \
    --container-name "my-javascript-runner-container" \
    --output-path "results_js.jsonl"
```

**3. Execute previously generated solutions from `generations_py.jsonl` in a Docker container:**
```bash
python bigcodebench_multipl/src/bigcodebench_multipl/run_benchmark.py execute \
    --container-name "my-python-runner-container" \
    --input-path "generations_py.jsonl" \
    --num-concurrent-requests 20 \
    --output-path "executions_py.jsonl"
```

## Dependencies and Interactions

- **External Libraries**:
  - `datasets`: For loading the BigCodeBench-MultiPL problem dataset.
  - `dspy-ai` (`dspy`): Core library for language model interaction (uses LiteLLM for broad model compatibility).
  - `bcb_multipl_util.extract_code_from_markdown`: Utility function to clean generated code from Markdown blocks.
  - `json`: For serializing/deserializing data for file I/O and Docker communication.
  - `argparse`: For parsing command-line arguments.
  - `asyncio`: Fundamental for the script's concurrent execution model.
  - `pathlib`: For object-oriented file system path manipulation.
  - `tqdm`: For displaying progress bars during generation.
- **Docker**: Essential for the execution phase. The script constructs and runs `docker run` commands. A compatible Docker container with the target language environment and an entrypoint that can process the JSON input is required.
- **Language Models**: Requires access to an LM configured via LiteLLM. API keys and environment variables for the chosen model must be set up correctly.
- **File System**:
  - Reads problem data from the Hugging Face `datasets` cache.
  - Reads generated solutions from an input file (for `execute` mode).
  - Writes results (generations and/or executions) to output files in JSONL format.
- **`bcb_multipl_util` module**: Depends on the `extract_code_from_markdown` function from this local module.
```
