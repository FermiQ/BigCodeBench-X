# make_stdio_problem.py

## Overview

The `make_stdio_problem.py` script is a command-line tool that leverages language models (LMs) via the DSPy library to convert traditional function-based programming problems from the BigCodeBench dataset into problems that are solved by standalone programs using standard input (STDIN) and standard output (STDOUT). This transformation is comprehensive, involving changes to the problem statement, the solution code structure, and the test suite format.

The script processes problems from BigCodeBench, applies the complex transformation using a specified LM, and outputs a new JSONL file containing these STDIN/STDOUT-based problem definitions.

## Key Components

### DSPy Signature

-   **`TranslateProblem(dspy.Signature)`**:
    -   Defines the core, multifaceted task for the language model.
    -   **Instruction/Prompt**: The LM is instructed to take an original problem statement, a Python function solution (`task_func`), and its associated test suite. It must then perform three transformations:
        1.  **Solution to Program**: Convert the provided Python function (`task_func`) into a complete, standalone Python program that reads input from STDIN and writes output to STDOUT. The LM should reuse code from the original function as much as possible.
        2.  **Problem Statement Update**: Modify the original problem statement to clearly ask for a standalone program with STDIN/STDOUT interaction, while retaining the original problem's essence and text where appropriate.
        3.  **Test Suite Rewrite**: Transform the original test suite into a new function with a specific signature: `def test_cases(prog: Callable[[str], Tuple[str, int]]): ...`. Inside this function, tests should call `prog(stdin_text)` which is expected to return a tuple `(stdout_text, exit_code)`. The new tests should be assertion-based and derived from the original tests.
    -   **InputFields**:
        -   `original_problem_statement: str`
        -   `original_program: str` (The code of the original Python function)
        -   `original_test_suite: str`
    -   **OutputFields**:
        -   `result_problem_statement: str` (The new problem statement for STDIN/STDOUT interaction)
        -   `result_program: str` (The new standalone Python program using STDIN/STDOUT)
        -   `result_test_suite: str` (The new test suite in the specified `test_cases` format)

### Helper Functions

-   **`prepare_dataset(original_dataset: Iterable[BigCodeBenchProblem]) -> List[dspy.Example]`**:
    -   **Purpose**: Converts raw `BigCodeBenchProblem` objects (loaded from `bcb_reader`) into `dspy.Example` objects suitable for processing by a DSPy module.
    -   **Processing**: Maps fields from `BigCodeBenchProblem` (like `problem`, `solution`, `tests`) to the corresponding `InputFields` of the `TranslateProblem` signature (`original_problem_statement`, `original_program`, `original_test_suite`).

### Core Workflow Functions

-   **`main_with_args(*, limit: Optional[int], model_name: str, temperature: float, max_tokens: int, batch_size: int, output_path: Path)`**:
    -   **Purpose**: Orchestrates the entire transformation process from loading data to saving results.
    -   **Processing**:
        1.  Initializes and configures the DSPy language model (`dspy.LM`) with `model_name`, `temperature`, and `max_tokens`.
        2.  Creates a `dspy.ChainOfThought` module (`translator`) using the `TranslateProblem` signature.
        3.  Loads the BigCodeBench dataset using `load_bigcodebench()` from `bcb_reader.py`.
        4.  If `limit` is specified, it truncates the list of problems to this number (useful for debugging).
        5.  Prepares the loaded problems into a list of `dspy.Example` objects using `prepare_dataset()`.
        6.  Opens the `output_path` file for writing.
        7.  Iterates through the original problems and the predictions generated by applying the `translator` module. This is done in batches using `incremental_parallel()` from `dspy_util.py`.
        8.  For each problem and its corresponding LM prediction:
            -   If the prediction is `None` (indicating an LM failure), an error object with the `task_id` is written to the output file.
            -   Otherwise, a dictionary is created containing the `task_id`, the LM's `reasoning` for the transformation, and the three transformed outputs: `prompt` (from `result_problem_statement`), `program` (from `result_program`), and `test_suite` (from `result_test_suite`). The `program` and `test_suite` fields are processed with `extract_code_from_markdown` to remove potential Markdown formatting from the LM's output.
            -   This dictionary is written as a JSON line to the `output_path`.
            -   The file buffer is flushed after each write to ensure data is saved progressively.

-   **`main()`**:
    -   **Purpose**: Parses command-line arguments and then calls `main_with_args()`.
    -   Uses `argparse.ArgumentParser` to define and manage all CLI arguments.

## Important Variables/Constants

The script's behavior is primarily controlled by command-line arguments:

-   **`--model-name` (str, default: "openai/o4-mini")**: The identifier for the language model to be used by DSPy (often compatible with LiteLLM).
-   **`--temperature` (float, default: 1.0)**: The sampling temperature for the LM, influencing the randomness/creativity of its output.
-   **`--limit` (int, optional, default: None)**: If set, limits the number of problems processed from the dataset. Useful for quick tests or debugging.
-   **`--max-tokens` (int, default: 20000)**: The maximum number of tokens the LM is allowed to generate for its comprehensive response (which includes the new problem, program, and test suite).
-   **`--batch-size` (int, default: 20)**: The number of problems to process in each parallel batch sent to the LM.
-   **`--output-path` (Path, required)**: The file path where the transformed STDIN/STDOUT-based problems (in JSONL format) will be saved.

**Key Output Fields**: The output JSONL objects will contain: `task_id`, `reasoning`, `prompt`, `program`, `test_suite`.

## Usage Examples

**Example: Transform the first 50 BigCodeBench problems into STDIN/STDOUT format using GPT-4, saving to `stdio_problems.jsonl`:**

```bash
# Ensure your OpenAI API key is set if using an OpenAI model, e.g.:
# export OPENAI_API_KEY="your_openai_api_key"

python bigcodebench_multipl/src/make_stdio_problem.py \
    --model-name "openai/gpt-4" \
    --temperature 0.8 \
    --batch-size 5 \
    --limit 50 \
    --output-path "./stdio_problems.jsonl"
```

## Dependencies and Interactions

-   **External Libraries**:
    -   `dspy`: The core library for all language model interactions, including defining the `TranslateProblem` signature and using `dspy.ChainOfThought`.
    -   `json` (Python standard library): Used for writing the output data in JSONL format.
    -   `argparse` (Python standard library): For parsing command-line arguments.
    -   `pathlib` (Python standard library): For object-oriented file path manipulation.
-   **Local Modules**:
    -   `bcb_reader` (`load_bigcodebench`, `BigCodeBenchProblem`): Essential for loading and structuring the input problems from the BigCodeBench dataset.
    -   `bcb_multipl_util` (`extract_code_from_markdown`): Used to clean up the code for the transformed program and test suite by removing any Markdown formatting potentially added by the LM.
    -   `dspy_util` (`incremental_parallel`): Used to manage the batched, parallel execution of the DSPy `translator` module over the input examples.
-   **Language Models (LMs)**: The script requires a functional LM accessible through DSPy. The quality and success of the transformation heavily depend on the capabilities of the chosen model.
-   **Input Data Source**: Relies on the BigCodeBench dataset as loaded by `bcb_reader.load_bigcodebench()`.
-   **Output Data Format**: Produces a JSONL file where each line is a JSON object representing a transformed problem, including its new STDIN/STDOUT-based problem statement, program, and test suite.
```
